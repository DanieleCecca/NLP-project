{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO0DvtOI/WFbvNSMwbfT3QE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanieleCecca/NLP-project/blob/main/FinetuningLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning LLM model with distillation\n",
        "*Daniele Cecca*\n",
        "\n",
        "*Matr. 914358*\n",
        "\n",
        "*MSc Artificial Intelligence for Science and Technology*\n",
        "\n",
        "*Email: d.cecca@campus.unimib.it*"
      ],
      "metadata": {
        "id": "JZsTQit-_gdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following notebook is useful to finetune an LLM model by using unsloth AI.\n",
        "We will train the model on the answer provided by gemini or gpt4, by using LoRA.\n",
        "\n",
        "For the momento thi finetunig is done by passing phrase by phrase.Another try that could be done in the future is to pass the entire context;the same that is passed to the bigger model.\n"
      ],
      "metadata": {
        "id": "plFr21hd_ltj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility"
      ],
      "metadata": {
        "id": "EvpITpu2ASRz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E4PJJFYW_fvG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRN7f_gFDRGz",
        "outputId": "3198afa3-c4c0-4d84-cb09-071afc883eff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import pandas as pd\n",
        "import json\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "from datasets import Dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "_oUDNRkcAaB4",
        "outputId": "07a652bf-23f4-4155-f809-e240e2d85d08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-09c2f79f898d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_templates\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_chat_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Fix Xformers performance issues since 0.0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functiona"
      ],
      "metadata": {
        "id": "huzvbQQmGMCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ],
      "metadata": {
        "id": "jS_tMhmyGOWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "I_cPBOuPBkWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH='/content/drive/MyDrive/Nlp/data/dataset_gemini.csv'\n",
        "TRAIN_PATH='/content/drive/MyDrive/Nlp/data/training_gemini.csv'\n",
        "TEST_PATH='/content/drive/MyDrive/Nlp/data/test_gemini.csv'"
      ],
      "metadata": {
        "id": "4oRmx5-OSHLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(TRAIN_PATH, encoding='unicode_escape')\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "S1xrBBVdFXu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''voglio che assegni ad ogni parola della frase una label, seguendo il formato IOB, usato per i task di NER(name entity recognition).\n",
        "Le entità sono le seguenti:\n",
        "-PROBLEMA\n",
        "-ESAME\n",
        "-FARMACI\n",
        "-OPERAZIONE\\n\n",
        "\n",
        "frase:\n",
        "'''"
      ],
      "metadata": {
        "id": "7ZmoMxwTMwrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['frase'] = df['frase'].apply(lambda x: prompt + x)"
      ],
      "metadata": {
        "id": "XaGtaxcFMlFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.frase[2]"
      ],
      "metadata": {
        "id": "l7PbVGKANPAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now use the Llama-3.1 format for conversation style finetunes.\n",
        "Llama-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "I'm great thanks!<|eot_id|>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."
      ],
      "metadata": {
        "id": "TyrFNMBJBma-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")"
      ],
      "metadata": {
        "id": "OCwD_0JyDDK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"conversations\"] = df.apply(\n",
        "    lambda x: [\n",
        "        {\"content\": x[\"frase\"], \"role\": \"user\"},\n",
        "        {\"content\": x[\"label\"], \"role\": \"assistant\"}\n",
        "    ], axis=1\n",
        ")\n",
        "\n",
        "# Ora convertiamo il DataFrame in un Dataset di HuggingFace, rimuovendo le vecchie colonne\n",
        "dataset = Dataset.from_pandas(df.drop(columns=[\"frase\", \"label\"]))"
      ],
      "metadata": {
        "id": "h_la269-FDae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "eO_nR0u8Fsse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['conversations'][0]"
      ],
      "metadata": {
        "id": "5ksEARp9F2ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n",
        "```\n",
        "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
        "{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n",
        "{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n",
        "```\n",
        "to\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```"
      ],
      "metadata": {
        "id": "mD2wQba6Ghpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "VCfNoPQcF9rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[5][\"conversations\"]"
      ],
      "metadata": {
        "id": "YtqgW7wTGX7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[5][\"text\"]"
      ],
      "metadata": {
        "id": "6Hz6HYOlGZHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "O1cNOBceBGHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model"
      ],
      "metadata": {
        "id": "ijYtbfg3BI5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "WPVtVDg6BFZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add LoRA"
      ],
      "metadata": {
        "id": "-L4SAHicBRkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "RMZE3wOEBUG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "44kbF5hfGtXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "7DLNKYcKG8nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
      ],
      "metadata": {
        "id": "TtPhlLhcHJNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "VRNAUmCrHJvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RYsF5DcgHa9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "6B2VqjVdHjs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show final memory and time stats"
      ],
      "metadata": {
        "id": "g2mxZossHrbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "trJjXC43HwNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Save model"
      ],
      "metadata": {
        "id": "DTpvJxyZIydQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Nlp/lora_llama\")  # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Nlp/lora_llama\")"
      ],
      "metadata": {
        "id": "hhiDt7IzI034"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference streaming without LoRA"
      ],
      "metadata": {
        "id": "4nIOkkElExPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ],
      "metadata": {
        "id": "tYNIxYDsKuwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_llama\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ],
      "metadata": {
        "id": "BQOusfJ3JJMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference streaming with LoRA"
      ],
      "metadata": {
        "id": "pAJbxZNYJg8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_llama\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Assegna ad ogni parola.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ],
      "metadata": {
        "id": "AzwvaKhWJcMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "wTvxKpyOSS6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utility functions"
      ],
      "metadata": {
        "id": "6oTu8-jbZz6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sendLLM(prompt_text,model='lora_llama'):\n",
        "    if True:\n",
        "      from unsloth import FastLanguageModel\n",
        "      model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "          model_name = model, # YOUR MODEL YOU USED FOR TRAINING\n",
        "          max_seq_length = max_seq_length,\n",
        "          dtype = dtype,\n",
        "          load_in_4bit = load_in_4bit,\n",
        "      )\n",
        "      FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"Assegna ad ogni parola.\"},\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
        "                            temperature = 1.5, min_p = 0.1)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "RNVPdNtQTihC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VEDERE output di risposta\n",
        "def create_dataset_IOB(list_sentences, prompt_base, model='lora_llama'):\n",
        "  data = []  # Lista per raccogliere i dati\n",
        "\n",
        "  for sentence in list_sentences:\n",
        "      prompt_text = prompt_base + sentence\n",
        "      response = sendLLM(prompt_text, model)  # Supponendo che questa funzione restituisca l'output corretto\n",
        "\n",
        "      data.append({'frase': sentence, 'label': response})  # Aggiungi la riga alla lista\n",
        "\n",
        "  dataset_IOB = pd.DataFrame(data)  # Crea il DataFrame una volta terminato il ciclo\n",
        "  return dataset_IOB"
      ],
      "metadata": {
        "id": "nAWqKDl_SnAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_iob_dataframe(df):\n",
        "    new_data = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        sentence = row['frase']\n",
        "        labels = row['label']\n",
        "\n",
        "        words = sentence.split()\n",
        "        label_list = labels.split()\n",
        "\n",
        "        if len(words) != len(label_list):\n",
        "            print(f\"Mismatch in row {index}, sentence '{sentence}'. Skipping this row.\")\n",
        "            continue\n",
        "\n",
        "        for i in range(len(words)):\n",
        "            new_data.append([index, words[i], label_list[i], sentence])\n",
        "\n",
        "    return pd.DataFrame(new_data, columns=[\"#Sentence\", \"Word\", \"Label\", \"Sentence\"])\n"
      ],
      "metadata": {
        "id": "wWg6Gih_ZxQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation"
      ],
      "metadata": {
        "id": "gbytgOAhZ4uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_base=\"\"\"Sei un medico e voglio che mi  assegni ad ogni parola una label, seguendo il formato IOB, usato per i task di NER.\n",
        "Le entità sono le seguenti:\n",
        "-PROBLEMA\n",
        "-ESAME\n",
        "-FARMACI\n",
        "-OPERAZIONE\n",
        "\n",
        "\n",
        "Con problema intendiamo anche malattie. Mentre con esame , intendiamo sia esame clinico che strumentale. Considera sia il nome dell'esame sia il valore-risultato dell'esame che puo' essere sia numerico che categorico. Lo stesso vale per i problemi per i farmaci e per le operazioni.\n",
        "\n",
        "Fai attenzione il numero di label deve essere uguale al numero di parole:\n",
        "O\n",
        "I_PROBLEMA\n",
        "B_PROBLEMA\n",
        "I_ESAME\n",
        "B_ESAME\n",
        "I_FARMACI\n",
        "B_FARMACI\n",
        "I_OPERAZIONE\n",
        "B_OPERAZION\n",
        "\n",
        "context:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "zxnNJ03sUBYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test=pd.read_csv(TEST_PATH, encoding='unicode_escape')\n",
        "list_sentences=df_test['frase']"
      ],
      "metadata": {
        "id": "YtzJagb9XoFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_prediction=create_dataset_IOB(list_sentences,prompt_base,model='lora_llama')"
      ],
      "metadata": {
        "id": "i6YPCCERUEMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=preprocess_iob_dataframe(df_test)['Label']\n",
        "prediction=preprocess_iob_dataframe(df_prediction)['Label']"
      ],
      "metadata": {
        "id": "5PlP6xFRY9DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "print(classification_report(labels, predictions))"
      ],
      "metadata": {
        "id": "Me_-_oUFYcar"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}